(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{287:function(v,_,t){"use strict";t.r(_);var i=t(5),s=Object(i.a)({},(function(){var v=this,_=v.$createElement,t=v._self._c||_;return t("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[t("h1",{attrs:{id:"网络爬虫-原理和预防"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#网络爬虫-原理和预防"}},[v._v("#")]),v._v(" 网络爬虫,原理和预防")]),v._v(" "),t("h2",{attrs:{id:"爬行原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#爬行原理"}},[v._v("#")]),v._v(" 爬行原理")]),v._v(" "),t("ul",[t("li",[v._v("模拟浏览器操作的过程。")]),v._v(" "),t("li",[v._v("爬虫访问网页的过程,就好比用户使用的浏览器")]),v._v(" "),t("li",[v._v("爬虫向页面发出访问请求,该页面的服务器则返回该页面的HTML代码")]),v._v(" "),t("li",[v._v("爬虫将收到的HTML代码存入搜索引擎的原始页面数据库中")])]),v._v(" "),t("h2",{attrs:{id:"如何爬行"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何爬行"}},[v._v("#")]),v._v(" 如何爬行")]),v._v(" "),t("p",[t("strong",[v._v("抓取网页")])]),v._v(" "),t("ul",[t("li",[v._v("选取一部分种子URL,将这些URL放入待抓取的URL队列")]),v._v(" "),t("li",[v._v("取出待抓取的URL,解析DNS得到主机的IP,并将URL对应的网页下载下来,存储进已下载网页库中,并且将这些URL放进已抓取URL队列")]),v._v(" "),t("li",[v._v("分析已抓取URL队列的URL,分析其中的其他URL,并且将URL放入待抓取URL队列，从而进入下一个循环")]),v._v(" "),t("li",[v._v("处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件")]),v._v(" "),t("li",[v._v("解析服务器响应内容")]),v._v(" "),t("li",[v._v("如何采集动态HTML、验证码的处理\n. 通用的动态页面采集、Selenium+Phantomjs（无界面）：模拟真实浏览器加载js、阿ajax非静态页面数据\n. tesseract:机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入，打码平台")]),v._v(" "),t("li",[v._v("Scrapy框架（Scrapy，Pyspider）\n高定制性高性能（异步网络框架twisted），所以数据下载速度特别快，提供数据存储、数据下载、提取规则等组件")]),v._v(" "),t("li",[v._v("分布式策略\n. scrapy redis,以scrapy的基础上添加了一套以redis数据库为核心的一台组件\n. 功能：主要在redis里做请求指纹去重、请求分配、数据临时存储")]),v._v(" "),t("li",[v._v("面向主题爬虫，面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求尽可能相关")])]),v._v(" "),t("p",[t("strong",[v._v("搜索引擎如何获取一个新网站的URL")])]),v._v(" "),t("ol",[t("li",[v._v("新网站向搜索引擎主动提交网址：（如百度http://zhanzhang.baidu.com/linksubmit/url）")]),v._v(" "),t("li",[v._v("在其他网站上设置新网站外链（尽可能处于搜索引擎爬虫爬取范围）")]),v._v(" "),t("li",[v._v("搜索引擎和DNS解析服务商(如DNSPod等）合作，新网站域名将被迅速抓取。")])]),v._v(" "),t("ul",[t("li",[v._v("网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取")])]),v._v(" "),t("p",[t("strong",[v._v("数据存储")]),v._v("\n搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。")]),v._v(" "),t("p",[v._v("搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。\n"),t("strong",[v._v("预处理")])]),v._v(" "),t("blockquote",[t("p",[v._v("搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。\n提取文字\n中文分词\n消除噪音（比如版权声明文字、导航条、广告等……）\n索引处理\n链接关系计算\n特殊文件处理\n....")])]),v._v(" "),t("h2",{attrs:{id:"反爬虫"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#反爬虫"}},[v._v("#")]),v._v(" 反爬虫")]),v._v(" "),t("ul",[t("li",[v._v("反爬虫：user agent、代理、验证码、动态数据加载、加密数据\n. 加上headers,不加请求头网站可以直接看出是程序在访问而直接拒绝,\n一般的网站加上User-Agent就可以,反爬严格的则要加上cookie甚至各种参数")]),v._v(" "),t("li",[v._v("随机延时,统计同一个IP一段时间内的访问频率,采集过快,直接封IP")]),v._v(" "),t("li",[v._v("单个IP快速访问会被封,可用代理池,")]),v._v(" "),t("li",[v._v("有两点好处：一是降低某个IP单位时间内的访问频率，降低被封风险；")]),v._v(" "),t("li",[v._v("二是即使IP被封，也有别的IP可以继续访问。代理池有免费和收费的，免费代理可以从许多网站上获取（这也是一个爬虫项目），但大部分都没用，有用的小部分也会很快挂掉;")]),v._v(" "),t("li",[v._v("有的网站必须要登录才能访问，才能爬虫")]),v._v(" "),t("li",[v._v("验证码,Python有自动识别图像的包，不过对于大部分网站的验证码")]),v._v(" "),t("li",[v._v("网站的反爬机制，各种JS代码逻辑十分复杂艰深")]),v._v(" "),t("li",[v._v("反爬虫主要还是看数据价值，是否值得去费尽做反爬虫")]),v._v(" "),t("li",[v._v("主要考虑如下方面：机器成本+人力成本>数据价值，就不反了，一般做到封IP就结束了")])])])}),[],!1,null,null,null);_.default=s.exports}}]);