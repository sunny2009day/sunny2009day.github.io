---
layout: post
title: 网络爬虫,原理和预防
categories: httpRequest
date: 2019-10-20 12:38:11
pid: 20191020-123811
pin: 100
---
## 爬行原理
- 模拟浏览器操作的过程。
- 爬虫访问网页的过程,就好比用户使用的浏览器
- 爬虫向页面发出访问请求,该页面的服务器则返回该页面的HTML代码
- 爬虫将收到的HTML代码存入搜索引擎的原始页面数据库中

## 如何爬行
**抓取网页**
- 选取一部分种子URL,将这些URL放入待抓取的URL队列
- 取出待抓取的URL,解析DNS得到主机的IP,并将URL对应的网页下载下来,存储进已下载网页库中,并且将这些URL放进已抓取URL队列
- 分析已抓取URL队列的URL,分析其中的其他URL,并且将URL放入待抓取URL队列，从而进入下一个循环
- 处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件
- 解析服务器响应内容
- 如何采集动态HTML、验证码的处理
 . 通用的动态页面采集、Selenium+Phantomjs（无界面）：模拟真实浏览器加载js、阿ajax非静态页面数据
 . tesseract:机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入，打码平台
- Scrapy框架（Scrapy，Pyspider）
  高定制性高性能（异步网络框架twisted），所以数据下载速度特别快，提供数据存储、数据下载、提取规则等组件
- 分布式策略
 . scrapy redis,以scrapy的基础上添加了一套以redis数据库为核心的一台组件
 . 功能：主要在redis里做请求指纹去重、请求分配、数据临时存储
 - 面向主题爬虫，面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求尽可能相关

**搜索引擎如何获取一个新网站的URL**
1. 新网站向搜索引擎主动提交网址：（如百度http://zhanzhang.baidu.com/linksubmit/url）
2. 在其他网站上设置新网站外链（尽可能处于搜索引擎爬虫爬取范围）
3. 搜索引擎和DNS解析服务商(如DNSPod等）合作，新网站域名将被迅速抓取。
- 网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取

**数据存储**
搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。

搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。
**预处理**
> 搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。
提取文字
中文分词
消除噪音（比如版权声明文字、导航条、广告等……）
索引处理
链接关系计算
特殊文件处理
....

## 反爬虫
- 反爬虫：user agent、代理、验证码、动态数据加载、加密数据
 . 加上headers,不加请求头网站可以直接看出是程序在访问而直接拒绝,
  一般的网站加上User-Agent就可以,反爬严格的则要加上cookie甚至各种参数
- 随机延时,统计同一个IP一段时间内的访问频率,采集过快,直接封IP
- 单个IP快速访问会被封,可用代理池,
 - 有两点好处：一是降低某个IP单位时间内的访问频率，降低被封风险；
 - 二是即使IP被封，也有别的IP可以继续访问。代理池有免费和收费的，免费代理可以从许多网站上获取（这也是一个爬虫项目），但大部分都没用，有用的小部分也会很快挂掉;
- 有的网站必须要登录才能访问，才能爬虫
- 验证码,Python有自动识别图像的包，不过对于大部分网站的验证码
- 网站的反爬机制，各种JS代码逻辑十分复杂艰深
- 反爬虫主要还是看数据价值，是否值得去费尽做反爬虫
- 主要考虑如下方面：机器成本+人力成本>数据价值，就不反了，一般做到封IP就结束了








 